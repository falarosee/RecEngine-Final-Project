---
title: "Recommendation Engines Final Project"
author: "Group 6/F"
date: "01/03/2019"
output:
  html_document:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}

# Load/Install packages

if(!"dplyr" %in% installed.packages()) {install.packages("dplyr")}
library(dplyr)

if(!"class" %in% installed.packages()) {install.packages("class")}
library(class)

if(!"recommenderlab" %in% installed.packages()) {install.packages("recommenderlab")}
library(recommenderlab)

if(!"shiny" %in% installed.packages()) {install.packages("shiny")}
library(shiny)
```

# Executive Summary:

## Project Aim & Introduction

This markdown contains the prototype and development of two recommendation engines, where one provides recommendations for groceries based on past transactions and the other provides recommendations for movies, based on user ratings. The final models were selected following the CRISP-DM framework for each business case: Business Understanding -> Data Understanding -> Data Preparation -> Modelling -> Model Evaluation, where special attention was given to the nature of each business problem for the model selection, prior to any performance evaluations. 

Given the separate nature, aim and construction of each recommendation engine, this markdown is split into two parts, where section 2. contains the technical methodology for the development of the groceries recommendation engine and section 3. contains the technical methodology and development for the movies recommendation engine.

The prototypes of the final models are stated in the following two sub-sections below. A detailed methodology of how these models were selected can be found in sections (2.4) for grocery recommendations and (3.4) for movie recommendations.

## Business Case & Prototype Description for Grocery Recommendations

The business objective of the recommendation engine for groceries is to increase sales of the online groceries store of the client, a major Spanish supermarket chain. An association rule model prototype was constructed to provide recommendations for groceries based on past customer transactions. Recommendations are based on computing rules that capture whether the purchase of one, or a group of grocery items purchased within a single transaction, increase the probability of purchasing another grocery item.

Within the deployment phase, the model is to be embedded within the online store and become part of the online shopping experience. Grocery item recommendations are made based on association rules that meet items that were already placed inside the shopping cart by a customer. Recommended items can be displayed via unobtrusive pop-ups that appear when a rule is met that passes a threshold that is to be set in accordance with the clients' business managers. 

Deployment of the model will overtime improve recommendations due to an implicit feedback loop, where new rules are created based on customer transactions and existing rules are strengthened by transactions that were affected by them. The latter happens when a customer accepts a recommendation based on a rule and proceeds to check out. This results in a further transaction that is added to the data which corresponds to-, and thereby reinforces the rule.

Thought can be given to implementing an explicit feedback loop, where the customer is given an option to rate a recommendation with a thumbs up or thumbs down. Alternatively, the direct acceptance of a recommendation can be considered as positive explicit feedback. These explicit ratings can be added as a further weight in the ranking of the recommended items.  

Thought was given to decide which association metrics (support, confidence, rule support, lift, confidence difference or confidence ratio) should be the decisive factor in setting the threshold for the deployment. According to best practices outlined by Lallich Et Al. (2005)[^1], lift, confidence difference and confidence ratio were deemed the most appropriate in comparing meaningful transaction driving association rules. Therefore, appropriate thresholds can be set for each metric in accordance with business experience during the deployment phase. For an explanation of the meaning of each metric, see **Appendix A**.

[^1]https://pdfs.semanticscholar.org/1671/eee808d8c8dfdb7a704962ca90b579cb8d1d.pdf 

The 20 most powerful rules according to the computation of lift, confidence difference and confidence ratio in the prototype can be explored below. To see the prototype, go to section 2.5 Model Prototype. 

### 20 Most Popular Rules according to Lift
 
 [1] {bottled beer,red/blush wine} => {liquor}                     
 [2] {hamburger meat,soda} => {Instant food products}              
 [3] {ham,white bread} => {processed cheese}                       
 [4] {root vegetables,other vegetables,whole milk,yogurt} => {rice}
 [5] {bottled beer,liquor} => {red/blush wine}                     
 [6] {Instant food products,soda} => {hamburger meat}              
 [7] {curd,sugar} => {flour}                                       
 [8] {soda,salty snack} => {popcorn}                               
 [9] {sugar,baking powder} => {flour}                              
[10] {processed cheese,white bread} => {ham}                       
[11] {ham,fruit/vegetable juice} => {processed cheese}             
[12] {margarine,sugar} => {flour}                                  
[13] {root vegetables,whole milk,sugar} => {flour}                 
[14] {soda,popcorn} => {salty snack}                               
[15] {flour,baking powder} => {sugar}                              
[16] {root vegetables,whole milk,butter} => {rice}                 
[17] {whipped/sour cream,sugar} => {baking powder}                 
[18] {whole milk,white bread,soda} => {processed cheese}           
[19] {ham,processed cheese} => {white bread}                       
[20] {whole milk,Instant food products} => {hamburger meat} 

### Most Popular Rules according to Confidence Difference

[1] {liquor,red/blush wine} => {bottled beer}                                            
[2] {citrus fruit,root vegetables,soft cheese} => {other vegetables}                     
[3] {pip fruit,whipped/sour cream,brown bread} => {other vegetables}                     
[4] {tropical fruit,grapes,whole milk,yogurt} => {other vegetables}                      
[5] {ham,tropical fruit,pip fruit,yogurt} => {other vegetables}                          
[6] {ham,tropical fruit,pip fruit,whole milk} => {other vegetables}                      
[7] {tropical fruit,butter,whipped/sour cream,fruit/vegetable juice} => {other vegetables}
[8] {whole milk,rolls/buns,soda,newspapers} => {other vegetables}                        
[9] {citrus fruit,tropical fruit,root vegetables,whipped/sour cream} => {other vegetables}
[10] {citrus fruit,other vegetables,soda,fruit/vegetable juice} => {root vegetables}      
[11] {tropical fruit,other vegetables,whole milk,yogurt,oil} => {root vegetables}         
[12] {root vegetables,butter,cream cheese } => {yogurt}                                   
[13] {tropical fruit,whole milk,butter,sliced cheese} => {yogurt}                         
[14] {other vegetables,curd,whipped/sour cream,cream cheese } => {yogurt}                 
[15] {tropical fruit,other vegetables,butter,white bread} => {yogurt}                     
[16] {other vegetables,whole milk,yogurt,rice} => {root vegetables}                       
[17] {tropical fruit,other vegetables,whole milk,oil} => {root vegetables}                
[18] {rice,sugar} => {whole milk}                                                         
[19] {canned fish,hygiene articles} => {whole milk}                                      
[20] {root vegetables,butter,rice} => {whole milk} 

### Most Popular Rules according to Confidence Ratio

 [1] {rolls/buns} => {specialty chocolate}                
 [2] {specialty chocolate} => {rolls/buns}                
 [3] {soda,specialty chocolate} => {other vegetables}     
 [4] {hamburger meat} => {soda}                           
 [5] {soda} => {hamburger meat}                           
 [6] {other vegetables,soda} => {canned beer}             
 [7] {beverages,soda} => {whole milk}                     
 [8] {grapes,rolls/buns} => {whole milk}                  
 [9] {brown bread,canned beer} => {whole milk}            
[10] {brown bread} => {coffee}                            
[11] {coffee} => {brown bread}                            
[12] {pip fruit} => {rolls/buns}                          
[13] {rolls/buns} => {pip fruit}                          
[14] {pork,frozen vegetables} => {soda}                   
[15] {beef,newspapers} => {soda}                          
[16] {root vegetables,yogurt,whipped/sour cream} => {soda}
[17] {other vegetables,yogurt} => {specialty chocolate}   
[18] {chewing gum} => {coffee}                            
[19] {pastry,soda} => {coffee}                            
[20] {coffee} => {chewing gum}  

For a detailed methodology of the model construction, see section 2.

## Business Case and Prototype Description for Movie Recommendations

The business objective of the recommendation engine for movies is to increase customer streaming time and improve customer experience of a the client, a major Spanish movie streaming platform. A hybrid model prototype, consisting of a collaborative filtering element, a random generator element and a popularity element has been developed to provide user-centric recommendations for movie streaming suggestions. Recommendations are generated based on past user ratings.  

Within the deployment phase, the model is to be embedded within the streaming platform and become part of the online streaming experience. Movie recommendations are made based on users with similar tastes (collaborative filtering), overall popularity and an element of randomness. Recommended movies can be displayed via a dedicated section on the user homepage and filtered according to dimensions such as genre and year.  

Deployment of the model can over time improve recommendations by implicit and explicit feedback loops. Implicit feedback is extracted through the collaborative filtering element of the hybrid model, where recommendations based on collaborative filtering are reinforced by users who watch recommended films. To avoid a circular feedback loop that continuously narrows the scope of the recommendations, an element of randomness is built into the model that occasionally recommends movies to the user that are outside of the user's usual viewing behavior. A circular feedback loop can occur when recommendations of movies that fit the user's viewing behavior get re-enforced and consequently teach the algorithm to recommend more of the same.

To improve recommendations over time further, it is recommended to implement an explicit feedback loop, where the customer is given an option to rate a recommended movie with a thumbs up or thumbs down, after having viewed it. This explicit user feedback adds information that helps to avoid the possibility of positive implicit feedback for movies that the user watched based on a recommendation, but didn't end up liking. The explicit feedback is therefore an important tool to correct a possible compounding of errors, caused by recommended movies the user watched and didn't like. 

The prototype of this model, along with the top 10 ratings for the first three users is displayed below. Thought was given to setting appropriate weights for the different elements of the hybrid, as well as setting an appropriate number of nearest neighbors. As users are not expected to watch many movies within a short time frame, the number of nearest neighbors was kept small. This enables the recommendation engine to start recommending without requiring a large number of watched movies for each user. The elements of the hybrid have been weighted according to their importance, where collaborative filtering is considered most important to tailor customized results (0.6), overall popularity is included to capture quick-win, popular movies and randomness (0.1) is kept small in order prevent scope narrowing, without significantly affecting recommendation quality.

The top 10 recommendations for the first two users, can be explored below.

### Top Recommendations for the First Two Users

Top 10 recommendations for user 1:

[[USER 1]]
 [1] "American Beauty (1999)"                     
 [2] "Schindler's List (1993)"                    
 [3] "Shawshank Redemption, The (1994)"           
 [4] "Dingo (1992)"                               
 [5] "Time of the Gypsies (Dom za vesanje) (1989)"
 [6] "Saving Private Ryan (1998)"                 
 [7] "Good Will Hunting (1997)"                   
 [8] "Requiem for a Dream (2000)"                 
 [9] "Cold Fever (� k�ldum klaka) (1994)"         
[10] "Gladiator (2000)"   

Top 10 recommendations for user 2:

[[USER 2]]
 [1] "Saving Private Ryan (1998)"               
 [2] "Matrix, The (1999)"                       
 [3] "Gladiator (2000)"                         
 [4] "Terminator 2: Judgment Day (1991)"        
 [5] "Godfather, The (1972)"                    
 [6] "Butch Cassidy and the Sundance Kid (1969)"
 [7] "Kelly's Heroes (1970)"                    
 [8] "Star Wars: Episode IV - A New Hope (1977)"
 [9] "Braveheart (1995)"                        
[10] "Annie Hall (1977)"

# Recommendation Engine For Groceries: Technical Methodology

## Data Exploration

The dataset provided for analysis contained grocery purchases pertaining to 9835 transactions and 169 grocery items, where each row represents a transaction and each column represents an item. Since there is no customer-identifier, it is impossible to know whether multiple transactions were executed by the same customer, or whether each transaction represents a single customer. 

The dataset was clean and ready for analysis upon receipt. The  169 variables were identified as binaries, as they have values of 0 and 1 to classify whether the item was purchased (1) or was not purchased (0) within the transaction represented by the row. 

First, the data is loaded from the recommenderlab package.

```{r load data}
data("Groceries")
```

### Item Distribution
```{r exploratory, echo=T}
groceries_summary <- summary(Groceries)
groceries_summary
```

```{r plots, echo=T}
plot(groceries_summary@lengths)
itemFrequencyPlot(Groceries, topN = 20)
```

The above plots show the distribution of the amount of items in every transaction. As can be seen above, a lot of the transactions contain exactly 1 item (2159 transactions). The number of transactions with x items decreases as x increases. On average, every transaction contains 4.409 unique items.

### Item Sparsity
```{r Sparsity}
image(sample(Groceries, 100))
```

The random sample of 100 transactions displayed above, confirms the sparsity of the dataset: only a few items are bought in every transaction and the majority of the items occur in a small part of the transactions. 

## Data Preperation

Having explored the data, it is now transformed into a binaryRatingMatrix, with which the recommender models can be built. Since the matrix doesn't contain any ratings, but rather binary values that indicate whether an item was part of a transaction or not, normalization is not applicable to the values.  

```{r Convert to Binary matrix}
groceriesDataset <- as(Groceries, "binaryRatingMatrix")
```

## Model Tuning

The following code lists all models within the recommenderlab that can be used to build recommendation scores on the given dataset. 

```{r Show all the possible algorithms for a binaryRatingMatrix, echo=TRUE}
recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")
```

As can be seen above, recommenderlab offers 6 recommendation algorithms for the groceries dataset, namely: Alternative Least Squares (ALS), Association Rules (AR), Item Based Collaborative Filtering (IBCF), Popular, Random and User Based Collaborative Filtering (UBCF). For a description of each of these algorithms, see **Appendix B**.

It is important to note here that the collaborative filtering UBCF and IBCF will not work exactly as intended, as a customer-identification dimension is missing from the dataset. Therefore, rather than building on the base assumption that groups of customers are inherently similar, it will treat transactions as customers and run on the assumption that groups of transactions are inherently similar. Nevertheless, this is accepted in the remaining section and all models will be explored based on their statistical validity. Subsequently, they will be evaluated according to their applicability for business. 

In order to evaluate for statistical validity, the data is first split into a train and test set, where 20% is reserved for the test set. As can be seen by the code below, the "given" parameter is set to -1, so that predicted recommendations can be compared to actual recommendations when the models are evaluated. 

```{r Split the data}
# 80-20 for train and test set
esSplitGroc <- evaluationScheme(groceriesDataset, method="split", train=0.80, given = -1)
```

Next, parameters are tuned in order to produce the highest score for each algorithm respectively.

### Tuning UBCF 

The UBCF algorithm needs an 'nn' parameter, which specifies the number of nearest neighbors that the algorithm should look at. We will make several UBCF algorithms with the parameter ranging from 100 to 1000, with a step of 100 to see which parameter value yields the best results. 

```{r Parameter tuning UBCF}

vector_nn <- seq(100,1000,100)
models_to_evaluate <- lapply(vector_nn, function(k){
  list(name = "UBCF",
       param = list(nn = k))
})
names(models_to_evaluate) <- paste0("UBCF_nn", vector_nn)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsIBCF, annotate=c(1,2))

```

As can be seen on the graph, the algorithm with nn = 400 performs the best, although there is not much difference between the algorithms. 

The model returns a list with N recommendations for every transaction in the test set. For N, the values 1, 5, 10, 15 and 20 are chosen, as appropriate bins for number of recommendations.

### Tuning IBCF

For IBCF, two parameters can be specified: the distance method to use and the parameter k, which is comparable to the parameter nn of UBCF and also specifies the neighborhood for comparison. The parameter k will vary between 100 and 250 in this case, with steps of 50, and we will compare the method 'Jaccard' and 'Cosine'.

```{r Parameter tuning IBCF}

vector_k <- c(100,150,200,250)
models_to_evaluateJac <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "Jaccard", k = k))
})
names(models_to_evaluateJac) <- paste0("IBCF_k_Jac", vector_k)

models_to_evaluateCos <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "cosine", k = k))
})
names(models_to_evaluateCos) <- paste0("IBCF_k_Cos", vector_k)

models_to_evaluate <- append(models_to_evaluateJac, models_to_evaluateCos)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))


plot(resultsIBCF, annotate=c(1,2))

```

For IBCF, we see that the Jaccard method works best, especially when less recommendations are made. In this case, it is much harder to detect the better parameter because they are all really comparable. We will choose 150 as the best parameter, although, again, the performance of the algorithms are very close for the different parameters.

## Model Selection

Parameters for AR are set to minimum, to generate the greatest amount of rules possible.

Parameters for ALS are set to default. 

Since there are no parameters to tune for the remaining algorithms, all algorithms mentioned above can now directly be compared with their optimal parameters, based on their respective performance. 

```{r Make the models}

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "UBCF" = list(name="UBCF", param=list(nn=400)),
  "IBCFJac" = list(name="IBCF", param=list(method = "Jaccard",
                                           k=150)),
  "ALS" = list(name = "ALS", param= list()),
  "AR" = list(name="AR", param = list(support = 0.001, conf = 0.01, maxlen=20))
)

resultsGroc <- evaluate(esSplitGroc, algorithms, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsGroc, annotate=c(1), legend = "topright")

```

As can be seen from the ROC plot above, the UBCF algorithm generates the best results, while ALS produces clearly the worst. The remaining algorithms yield similar results. 

Judging purely from a statistical performance view, UBCF should be selected as final model. As mentioned previously however, UBCF and IBCF are collaborative filtering methods built on the assumption that groups of customers are inherently similar, not that groups of transactions are inherently similar. The problem of using collaborative filtering in this context is that if only few customers made most of the purchases that were peculiar to the given dataset, the assumption that the transactions are inherently similar no longer stands and cannot generalize beyond the given dataset. 

As the popular algorithm recommends only the most popular items and would therefore generate the same recommendations over and over again, the AR algorithm is left as most appropriate choice for the above stated business case. 

## Final Model Prototype - Groceries

When training the AR model, we set the parameters to minimum to generate the greatest amount of rules. A great amount of rules is desirable for the business case, as it will be able to capture niche customer behavior, and recommend items accordingly. 

```{r check AR rules, echo=TRUE, }

groceriesRecAr2 <- Recommender(groceriesDataset, method = "AR", parameter = list(support = 0.001, conf = 0.001, maxlen = 50))

arGroceriesModel <- getModel(groceriesRecAr2)$rule_base
summary(arGroceriesModel)
rulesDF <- as(arGroceriesModel, "data.frame")
rulesDF$prior_conf <- rulesDF$confidence / rulesDF$lift
rulesDF$confidence_diff <- abs(rulesDF$confidence - rulesDF$prior_conf)
rulesDF$confidence_ratio <- 1- (apply(rulesDF[,c(3,7)], 1, min) / apply(rulesDF[,c(3,7)], 1, max) )

# Top 20 according to each AR metric
head(rulesDF[order(rulesDF$lift, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_ratio, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_diff, decreasing = T),],20) # Rules with longer LHS usually have higher confidence_difference
head(rulesDF[order(rulesDF$confidence, decreasing = T),],20) 
head(rulesDF[order(rulesDF$support, decreasing = T),],20)
```


# Recommendation Engine For Movies: Technical Methodology

## Data Exploration

The dataset provided for analysis represents a rating matrix with 999,714 ratings from 6,040 users to 3,449 movies. The ratings span is from 1 to 5.

First, the data is loaded from an external source.

```{r}
load("bigMovieLense.RData")
```

As an initial step, the distribution of Ratings will be analyzed.

```{r}
str(bigMovieLense)
```

From the code above, we can see that on average one user has evaluated 166 movies. The user that has the minimum number of evaluations has only 18, while the user with the maximum number has 2,283.

Considering that some users tend to give low or high ratings to all of their movies, the results may be biased. To remove this effect, the data must be normalized in such a way that the average rating of each user is 0. 

The following histograms show the distribution of ratings from users. We can see that the ratings are slightly skewed to the left, indicating that most of the movies are rated with 4/5.

```{r}

# On average wow many evaluations per user we have?
mean(rowCounts(bigMovieLense))
min(rowCounts(bigMovieLense))
max(rowCounts(bigMovieLense))

# Which are the evaluations from the first user?
#as(bigMovieLense[1,], "list")

# On average, how much the first user rates?
rowMeans(bigMovieLense[1,])

# Distribution of the ratings
hist(getRatings(bigMovieLense), col = "darkred")
hist(getRatings(normalize(bigMovieLense, method = "Z-score")), main="Z-score Ratings Distribution", col = "Red")

#distribution of how many movies each user has rated
hist(rowCounts(bigMovieLense), breaks = 50, main = "Histogram Rated movies by user", col = "lightBlue")

# Distribution of the average ratings per movie
hist(colMeans(bigMovieLense), main = "Histogram Average Rating per Movie", col = "lightGreen")
```

In the following Rating Matrix Heat map we subset the first 50 users and 100 movies. We can see that the matrix is highly sparse and some users tend to evaluate in a more aggressive/non-aggressive way. This supports the fact that the information has to be normalized for a correct analysis. 

```{r}

image(getRatingMatrix(bigMovieLense[1:50,1:100]), main="Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

image(getRatingMatrix(normalize(bigMovieLense, method="Z-score")[1:50,1:100]), main=" Normalized Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

```

Now we will analyze which are the most viewed movies (based in the number of ratings) and the top average rated. We can see there is no clear relation between total views and top rated.

```{r}

views_per_movie <- colCounts(bigMovieLense)
table_view <- data.frame(movie = names(views_per_movie), views = views_per_movie)
table_view <- table_view[order(table_view$views, decreasing = T),]

avg_rating_pre_movie <- colMeans(bigMovieLense)
top_rated <- data.frame(movie = names(avg_rating_pre_movie), rating = avg_rating_pre_movie)
top_rated <- top_rated[order(top_rated$rating, decreasing = T),]

avg_movies <- merge(table_view, top_rated, by = "movie")
top_views <- avg_movies[order(avg_movies$views, decreasing = T),]
top_rated <- avg_movies[order(avg_movies$rating, decreasing =T),]

# Top viewed movies
print(as(top_views[1:10,],"matrix"))

# Top rated movies
print(as(top_rated[1:10,],"matrix"))

plot(avg_movies$rating, avg_movies$views, main = "Ratings vs Views", xlab="Movie Average Rating", ylab = "Total Views", col="lightblue")

```

## Model Tuning & Selection

### Evaluation Scheme

We create an evaluation scheme with 80% training set and 20% testing. For the test, 5 items will be given to the recommender algorithm and the others will be held out for computing the error. In this case having a small "given" parameter makes sense because the movie consumption is not as frequent as for example songs. We would only need five movie ratings by the user to start recommending. 

Considering that the ratings are distributed from 1 to 5, we assume a good rating of 4 and 5. This binarization metric is important for understanding the popularity of an item. 

We may perform a K-fold cross evaluation to better compute the algorithm performance indicators. However, to optimize the running time, a single split is taken into account.

### Algorithms

We will test recommendation systems with the following algorithms: random, popularity, UBCF, UBCF Pearson ALS and SVD. For a complete description of each of these algorithms see **appendix B**.  

```{r}
# Get the different Algorithms that we may apply to our Recommendation Engine
recommenderRegistry$get_entries(dataType = "realRatingMatrix")

MoviesSubset <- bigMovieLense[1:(nrow(bigMovieLense)*0.4),]

set.seed(123)
Multi_evaluate <- evaluationScheme(MoviesSubset, method = "split", train=0.8, given = 5, goodRating = 4)

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFCos" = list(name="UBCF", param=list(method = "Cosine")),  
  "Item Based CFPea" = list(name="IBCF", param=list(method = "Pearson")),
  "ALS" = list(name="ALS", param=list(k=50)),
  "SVD" = list(name="SVD", param=list(k=50))
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

ratings_results <- evaluate(Multi_evaluate, algorithms, type = "ratings")

plot(ratings_results)
```

In the previous plots we can see the best results come from the *POPULARITY* algorithm.

In the ROC curve, we find a steep increase in True Positive Rates until the tenth recommendation. We would not recommend more than 10 movies considering a possible bigger marginal increment in False Positive Rates.

It is important to consider that the POPULARITY algorithm is based in the most popular items. From a business perspective, this may affect negatively the recommendations incurring in the banana problem (Only recommends top rating items). Considering this, we would create a Hybrid Recommendation Engine that includes *Popularity* and *User Based* Recommendations with a small number of nearest neighbors.

### Exploring the UBCF Parameters

If we increase the number of Nearest Neighbors in the UBCF algorithm, we see that the AUC improves, with a tendency towards the Popularity algorithm. This happens because we are considering a larger number of neighbors tending towards the usage of the complete users in the dataset. From a business perspective we would like to have a balance in the recommendations of popular items with personalized items, considering only what the most similar reduced set of neighbors prefer. This may drop the accuracy level of the algorithm but consider more recommendations that will impact positively the business.

```{r}
algorithms2 <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn10" = list(name="UBCF", param=list(nn=10)),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFnn100" = list(name="UBCF", param=list(nn=100)),
  "User Based CFnn200" = list(name="UBCF", param=list(nn=200))
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms2, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")
```

## Final Model Prototype: Movies

Our hybrid recommendation engine proposal is more weighted towards the User Based Collaborative Filtering (70%) than towards the popularity (30%). We want to give more importance to recommend a broader set of movies suited to user preferences. 

```{r}
MoviesTrainDS <- bigMovieLense[1:(nrow(bigMovieLense)*0.8),]
MovieTestDS <- bigMovieLense[4833:nrow(bigMovieLense),]

#Recommender of type POPULAR 
Popular_recMovies <- Recommender(MoviesTrainDS, method="POPULAR")
# Recommender UBCF
UBCF_recMovies <- Recommender(MoviesTrainDS, method = "UBCF", parameter = list(nn=10))
# Random Recommender
Random_recMovies <- Recommender(MoviesTrainDS, method = 'RANDOM')
# Hybrid Recommender
hybrid_rec <- HybridRecommender(Popular_recMovies, UBCF_recMovies, Random_recMovies ,weights = c(0.3,0.6,0.1))
```

### Recommending Movies

Now that we created a Hybrid recommender merging Popular and User Based CF, we will compare the results of the 10 recommended movies: 

* The first list displays the Hybrid Recommender 30% Popularity and 70% UBCF.
* The second list displays a 100% Popularity-based recommender.
* The third list displays a 100% UBCF with 10 nearest neighbors.

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict <- predict(hybrid_rec, newdata = MovieTestDS[1,], n=10)
as(hybrid_predict, "list")

Popular_predict <- predict(Popular_recMovies, newdata = MovieTestDS[1,], n=10)
as(Popular_predict, "list")

UBCF_predict <- predict(UBCF_recMovies, newdata = MovieTestDS[1,], n=10)
as(UBCF_predict, "list")
```

We can see that inside the 10 recommended movies of our Hybrid Algorithm we find some of the Highest Viewed / Highest Rated movies such as American Beauty but also movies more suited to the user's preferences. This also happens to another randomly selected user:

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict2 <- predict(hybrid_rec, newdata = MovieTestDS[200,], n=10)
as(hybrid_predict2, "list")

Popular_predict2 <- predict(Popular_recMovies, newdata = MovieTestDS[200,], n=10)
as(Popular_predict2, "list")

UBCF_predict2 <- predict(UBCF_recMovies, newdata = MovieTestDS[200,], n=10)
as(UBCF_predict2, "list")

```

# Conclusion and Next Steps

In this markdown report, we have presented two model prototypes that can be deployed to server their respective business needs. To ensure sustained model robustness, we recommend to continually evaluate model performance within both business cases during and after the deployment phase. 

To improve rankings for the grocery model, we suggest adding a customer identification dimension, to be able to successfully apply user based collaborative filtering.

# Bibliography

Lallich (2005) “Association rule interestingness: measure and statistical validation”: https://pdfs.semanticscholar.org/1671/eee808d8c8dfdb7a704962ca90b579cb8d1d.pdf

Jiawei Han and Micheline Kamber, “Data Mining:  Concepts and Techniques”, 2000

Vipin Kumar and Mahesh Joshi, “Tutorial on High Performance Datamining”, 1999

Rakesh Agrawal, Ramakrishnan Srikan, “Fast Algorithms for Mining Association Rules”, Proc VLDB, 1994 

Statistics How To: https://www.statisticshowto.datasciencecentral.com/market-basket-analysis/

# Appendix

## Appendix A: Description of Association Metrics

* *Lift* --> The lift ratio represents how likely item Y is purchased when item X is purchased, controlling for how popular item Y is. It is important to note that:
●	A lift value > 1 means that item Y is likely to be bought if item X is bought.
●	A lift value <1 means that item Y is unlikely to be bought if item X is bought.
A lift value nearly equal to 1 indicates that if item X is bought, there is no bearing on the purchase of item Y. 

* *Confidence Difference* --> The confidence difference is based on the absolute difference between the Posterior and Prior Confidence. This rule is particularly useful, as the resulting absolute value can help indicate negative rules (rules used to eliminate events). Confidence Ratio helps compare confidences by calculating their ratio and subtracting it from 1.  This metric is useful, as it can detect negative rules as well as rarities.

* *Confidence Ratio* --> Confidence Ratio helps compare confidences by calculating their ratio and subtracting it from 1.  This metric is useful, as it can detect negative rules as well as rarities. Further, it is sensitive to lower confidence figures. For example, rule 1: 10% prior and 20% posterior. Rule 2: 30% prior and 40% posterior. They have identical confidence difference of 10%. However, rule 1 has higher confidence ratio of 0.50 (1-(½)) versus rule 2 of 0.25 (1-(¾)).

## Appendix B: Description of Recommendation Algorithms

* *Random* --> Produces random recommendations. This algorithm serves as a benchmark to confirm that the algorithms give a better prediction than a random recommended.

* *Popularity* --> Recommendation based on items popularity. This algorithm is not customized to individual users or transactions. 

* *UBCF* --> The user-based CF assumes that similar users tend to behave in a similar fashion, that is they have similar tastes. There are different similarity measures between users. Two of them are nearest neighbors (nn=50) and cosine similarity.
** User Based Collaborative Filtering (UBCF) NN = 50 --> The recommendations are given based in the 50 nearest neighbors.
** User Based Collaborative Filtering (UBCF) Cosine --> The recommendations are given based in the cosine similarity measure. 

* *IBCF Pearson* --> The item-based CF recommends items based on their similarity with the items that the target user rated. The advantage of this algorithm family is that it tackles the issue that users' preferences may change over time. For this exercise we test the Pearson similarity measure. We can also use the Cosine or Jaccard similarity measures. However, the Pearson correlation is invariant to adding a constant to all elements.

* *Alternating Least Squares Matrix Factorization (ALS) & Singular Value Decomposition (SVD)*

The ALS and SVD algorithms are matrix factorization solutions for collaborative filtering, when we need to scale the recommendation system. We will test the ALS and SVD algorithms assuming that we would require scalability in the future.

