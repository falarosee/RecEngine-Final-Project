---
title: "Recommendation Engines Final Project"
author: "Franz-Anton La Rosee"
date: "01/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}

# Load/Install packages

if(!"dplyr" %in% installed.packages()) {install.packages("dplyr")}
library(dplyr)

if(!"class" %in% installed.packages()) {install.packages("class")}
library(class)

if(!"recommenderlab" %in% installed.packages()) {install.packages("recommenderlab")}
library(recommenderlab)

if(!"shiny" %in% installed.packages()) {install.packages("shiny")}
library(shiny)
```

# 1. Executive Summary:

## 1.1 Project Aim & Model Results

This markdown contains the development of two recommendation engines, where one provides recommendations for groceries based on past transactions and the other provides recommendations for movies, based on user ratings. The final models were selected following the CRISP-DM framework for each business case: Business Understanding -> Data Understanding -> Data Preperation -> Modelling -> Model Evaluation, where special attention was given to the nature of each business problem for the model selection, prior to any performance evaluations. 

According to this approach, the following two models were selected, with their corresponding results:

### Final Model - Recommendation Engine For Groceries

To provide recommendations for groceries based on past transactions an association analysis was performed to identify rules that define whether the purchase of one or more groceries would increase the probability of a purchase of another grocery item. To see the methodology of how this model was selected, see Section 2.4, model selection. 

Amongst the many rules discovered, BLANK distinct rules have been identified in terms of statistical validity and value for business. 

Insights into purchasing patterns gained from these rules are intended to guide the business manager to increase grocery sales, by discovering item groups that customers tend to buy together.The analysis herein utilized sales information contained in the dataset “Groceries”. 
By utilizing the associative metrics Lift, Confidence difference and Confidence ratio, the following set of BLANK rules was identified:


### Model Results - Recommendation Engine For Movies

Given the seperate nature, aim and construction of each recommendation engine, the model development section of this markdown is split into two parts, where section 2.1 contains the development for the groceries recommendation engine and section 2.2 contains the development for the movies recommendation engine. 

# 2. Model Development: Recommendation Engine For Groceries

## 2.1 Business Case

## 2.2 Data Exploration

Load the groceries dataset, and transform it to a binaryRatingMatrix, from the recommenderlab package.

```{r load data}
data("Groceries")

```

```{r exploratory}
groceries_summary <- summary(Groceries)
groceries_summary
plot(groceries_summary@lengths)
itemFrequencyPlot(Groceries, topN = 20)

```

The Groceries dataset consists of 169 columns and 9835 rows, that respectively represent the different items and the different transactions.
The dataset represents different transactions (baskets), so the recommendation algorithm that comes to mind here is Association Rules.

With the help of some plots, we explore the groceries dataset.
First of all, we plot the distribution of the amount of items in every transaction. As we can see on the plot, a lot of the transactions contain exactly 1 item (2159 transactions). The number of transactions with x items decreases as x increases. On average, every transaction contains 4.409 unique items.

A random sample of 100 transactions for the groceries dataset confirms the sparsity of the dataset: only a few items are bought in every transaction and every item only occurs in a few transactions.

```{r Sparsity}

image(sample(Groceries, 100))

```

## 2.3 Data Preperation

The groceries dataset is actually a binary matrix. This matrix doesn't contain any real ratings, so normalization is not possible in this case. We just convert the transactions 'Groceries' to a binaryRatingMatrix 'groceriesDataset'. With this binaryRatingMatrix we can build our recommenders.

```{r Convert ot Binary matrix}

groceriesDataset <- as(Groceries, "binaryRatingMatrix")

```

## 2.4 Model Selection

The following output shows all the possible algorithms for a binaryRatingMatrix.

```{r Show all the possible algorithms for a binaryRatingMatrix}

recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")

```

There are six possible recommendation algorithms for the groceries dataset, namely: Alternative Least Squares (ALS), Association Rules (AR), Item Based Collaborative Filtering (IBCF), Popular, Random and User Based Collaborative Filtering (UBCF).
We are going to take a look at the AR, IBCF, UBCF and Popular algorithms, and we are going to use the Random algorithm as a baseline.

First we are going to compare these algorithms according to their ROC curve. We split the data in train and test data. Then we evaluate their performance with a model that was trained on the train set (one model for every algorithm). The model then returns a list with N recommendations for every transaction in the test set. For N, we choose the values 1, 5, 10, 15 and 20 because ultimately it doesn't make sense to recommend more than 20 grocery store items. For the 'given' parameter in the evaluationScheme function we chose -1, that way the algorithms get all but one items from a transaction and they try to predict the one extra item in that transaction. This makes sense because there are not that many items in each transaction, so it's easier to make a prediction for one item, on the basis of the rest of the transaction. From a business perspective this also makes sense because it is usually the case that a recommendation is done, according to the items that are already in your basket (in online shopping). 

```{r Split the data}

# 80-20 for train and test set
esSplitGroc <- evaluationScheme(groceriesDataset, method="split", train=0.80, given = -1)

```

For the parameter tuning of the different algorithms, we only need to look at the UBCF, IBCF and the AR algorithms because the popular and random ones don't need any parameters.
The UBCF algorithm needs an 'nn' parameter, which specifies the number of nearest neighbours that the algorithm should look at. We will make several UBCF algorithms with the parameter ranging from 100 to 1000, with a step of 100 to see which parameter value yields the best results. 
In this case, for groceries, there are no actual users, because the rows in the groceries matrix represent transactions and not users. So User Based CF is actually Transaction Based CF in this case. It looks for similar transactions and then recommends items that were present in these similar transactions.

```{r Parameter tuning UBCF}

vector_nn <- seq(100,1000,100)
models_to_evaluate <- lapply(vector_nn, function(k){
  list(name = "UBCF",
       param = list(nn = k))
})
names(models_to_evaluate) <- paste0("UBCF_nn", vector_nn)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsIBCF, annotate=c(1,2))

```

As we can see on the graph, the algorithm with nn = 400 performs the best, although there is not much difference between the algorithms. 

For IBCF, we can specify two parameters: the distance method to use and the parameter k, which is comparable to the parameter nn of UBCF and also specifies the neighbourhood for comparison. 
The parameter k will vary between 100 and 250 in this case, with steps of 50, and we will compare the method 'Jaccard' and 'Cosine'.

```{r Parameter tuning IBCF}

vector_k <- c(100,150,200,250)
models_to_evaluateJac <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "Jaccard", k = k))
})
names(models_to_evaluateJac) <- paste0("IBCF_k_Jac", vector_k)

models_to_evaluateCos <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "cosine", k = k))
})
names(models_to_evaluateCos) <- paste0("IBCF_k_Cos", vector_k)

models_to_evaluate <- append(models_to_evaluateJac, models_to_evaluateCos)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))


plot(resultsIBCF, annotate=c(1,2))

```

For IBCF, we see that the Jaccard method works best, especially when less recommendations are made. In this case, it is much harder to detect the beter parameter because they are all really comparabl. We will choose 150 as the best parameter, although, again, the performance of the algorithms are very close for the different parameters.

Now we compare all these algorithms, with their optimal parameters, with each other. For AR we choose our parameters in such a way that enough rules are generated to make enough recommendations so we can evaluate it easily with the other algorithms.

```{r Make the models}

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "UBCF" = list(name="UBCF", param=list(nn=400)),
  "IBCFJac" = list(name="IBCF", param=list(method = "Jaccard",
                                           k=150)),
  "AR" = list(name="AR", param = list(support = 0.001, conf = 0.01, maxlen=20))
)

resultsGroc <- evaluate(esSplitGroc, algorithms, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsGroc, annotate=c(1), legend = "topright")

```

This plot shows that the UBCF algorithm gives the best results. The rest of the algorithms actually yield similar results. For UBCF and IBCF, it is the case that for a growing parameter nn and k respectively the algorithms get similar to the 'popular' algorithm because it takes more neighbours into account and ultimately it will take every User/Item into account, just like the 'popular' algorithm does.
The UBCF algorithm yields the best results, however from a business perspective it is not the most interesting algorithm. Mainly because it doesn't really give an insight in how several items are related, it just makes recommendations based on items that are similar. It is of more use to us to know the items that get frequently bought together and have some kind of rules that explain these relationships between items.
Because the difference between the algorithms isn't that big, we choose to advance with the AR algorithm because it makes the most sense in this business case and it is easier to draw conclusions from these rules.

## 2.5 Model Tuning

In this section, we are going to tune the parameter for our AR algorithm.

```{r check AR rules}

groceriesRecAr <- Recommender(getData(esSplitGroc,"train"), method = "AR", parameter = list(support = 0.001, conf = 0.2, maxlen = 5))

arGroceriesModel <- getModel(groceriesRecAr)$rule_base
summary(arGroceriesModel)
rulesDF <- as(arGroceriesModel, "data.frame")
rulesDF$prior_conf <- rulesDF$confidence / rulesDF$lift
rulesDF$confidence_diff <- abs(rulesDF$confidence - rulesDF$prior_conf)
rulesDF$confidence_ratio <- 1- (apply(rulesDF[,c(3,7)], 1, min) / apply(rulesDF[,c(3,7)], 1, max) )

# Top 20 according to each AR metric
head(rulesDF[order(rulesDF$lift, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_ratio, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_diff, decreasing = T),],20) # Rules with longer LHS usually have higher confidence_difference
head(rulesDF[order(rulesDF$confidence, decreasing = T),],20) 
head(rulesDF[order(rulesDF$support, decreasing = T),],20) 

```


# 3. Model Development: Recommendation Engine For Movies

```{r}
load("bigMovieLense.RData")
```

The following document shows the analysis of different recommendation algorithms applied to the _Big Movie Lense dataset_. The main objective is to apply variations of recommendation algorithms to understand which performs better and why.

As we can see, the dataset is a rating matrix with 999,714 ratings from 6,040 users to 3,449 movies. The ratings span is from 1 to 5.

```{r}
str(bigMovieLense)
```

# Descriptive analysis

As an initial step, we will analyze the distribution of Ratings.
We can see that on average one user has evaluated 166 movies.
The user that has the minimum number of evaluations has only 18, while the user with the maximum number has 2,283.

Considering that some users tend to give low or high ratings to all of their movies, the results may be biased. To remove this effect, the data must be normalized in such a way that the average rating of each user is 0. 

The following histograms show the distribution of ratings from users. We can see that the ratings are slightly skewed to the left, indicating that most of the movies are rated with 4/5.


```{r}

# On average wow many evaluations per user we have?
mean(rowCounts(bigMovieLense))
min(rowCounts(bigMovieLense))
max(rowCounts(bigMovieLense))

# Which are the evaluations from the first user?
#as(bigMovieLense[1,], "list")

# On average, how much the first user rates?
rowMeans(bigMovieLense[1,])

# Distribution of the ratings
hist(getRatings(bigMovieLense), col = "darkred")
hist(getRatings(normalize(bigMovieLense, method = "Z-score")), main="Z-score Ratings Distribution", col = "Red")

#distribution of how many movies each user has rated
hist(rowCounts(bigMovieLense), breaks = 50, main = "Histogram Rated movies by user", col = "lightBlue")

# Distribution of the average ratings per movie
hist(colMeans(bigMovieLense), main = "Histogram Average Rating per Movie", col = "lightGreen")
```

In the following Rating Matrix Heat map we subset the first 50 users and 100 movies. We can see that the matrix is highly sparsed and some users tend to evaluate in a more aggressive/non-aggressive way. This supports the fact that the information has to be normalized for a correct analysis. 

```{r}

image(getRatingMatrix(bigMovieLense[1:50,1:100]), main="Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

image(getRatingMatrix(normalize(bigMovieLense, method="Z-score")[1:50,1:100]), main=" Normalized Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

```

Now we will analyze which are the most viewed movies (based in the number of ratings) and the top average rated. We can see there is not a commonality of the most view with the top rated.

```{r}

views_per_movie <- colCounts(bigMovieLense)
table_view <- data.frame(movie = names(views_per_movie), views = views_per_movie)
table_view <- table_view[order(table_view$views, decreasing = T),]

avg_rating_pre_movie <- colMeans(bigMovieLense)
top_rated <- data.frame(movie = names(avg_rating_pre_movie), rating = avg_rating_pre_movie)
top_rated <- top_rated[order(top_rated$rating, decreasing = T),]

avg_movies <- merge(table_view, top_rated, by = "movie")
top_views <- avg_movies[order(avg_movies$views, decreasing = T),]
top_rated <- avg_movies[order(avg_movies$rating, decreasing =T),]

# Top viewed movies
print(as(top_views[1:10,],"matrix"))

# Top rated movies
print(as(top_rated[1:10,],"matrix"))

plot(avg_movies$rating, avg_movies$views, main = "Ratings vs Views", xlab="Movie Average Rating", ylab = "Total Views", col="lightblue")

```

# Comparing Recommendation Algorithms

### Evaluation Scheme

We create an evaluation scheme with 80% training set and 20% testing. 
For the test, 5 items will be given to the recommender algorithm and the others will be held out for computing the error. In this case having a small "given" parameter makes sense because the movie consumption is not as frequent as for example songs. We would only need five movie ratings by the user to start recomending. 

Considering that the ratings are distributed from 1 to 5, we assume a good rating of 4 and 5. This binarization metric is important for understandig the popularity of an item. 

We may perform a K-fold cross evaluation to better compute the algorithm performance indicators. However, to optimize the running time, a single split is taken into account.

### Algorithms

We will test recommendation systems with the following algoriths:

* *Random* --> Produce random recommendations. This algorithm serves as a benchmark to confirm that the algorithms give a better prediction than a random recommender.
* *Popularity* --> Recomendation based in items popularity.

* *UBCF* --> The user-based CF assumes that similar users tend to give similar rates. There are different similarity measures between users. For this exercise we will test the Nearest Neighbors (nn=50) and the Cosine similarity measure.

** User Based Collaborative Filtering (UBCF) NN = 50 --> The recommendations are given based in the 50 nearest neighbors.
** User Based Collaborative Filtering (UBCF) Cosine --> The recommendations are given based in the cosine similarity measure. 

* *IBCF Pearson* --> The item-based CF recommends items based on their similarity with the items that the target user rated. The advantage of this algorithm family is that it tackles the issue that users' prefrences may change over time. For this exercise we test the Pearson similarity measure. We can also use the Cosine or Jaccard similarity measures. However, the Pearson correlation is invariant to adding a constant to all elements.

* *Alternating Least Squares Matrix Factorization (ALS)* --> In collaborative filtering, the scalability issue is common with large datasets. If we add more users and movies, the matrix grows exponentially, and the majority of movies receive few or even no ratings ending with a really sparse matrix. The ALS algorithm is a matrix factorization solution for collaborative filtering, when we need to scale the recommendation system. We will test the ALS algorithms assuming that we would require scalability in the future.


```{r}

# Get the different Algorithms that we may apply to our Recommendation Engine
recommenderRegistry$get_entries(dataType = "realRatingMatrix")

MoviesSubset <- bigMovieLense[1:(nrow(bigMovieLense)*0.4),]

set.seed(123)
Multi_evaluate <- evaluationScheme(MoviesSubset, method = "split", train=0.8, given = 5, goodRating = 4)

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFCos" = list(name="UBCF", param=list(method = "Cosine")),  
  "Item Based CFPea" = list(name="IBCF", param=list(method = "Pearson")),
  "ALS" = list(name="ALS", param=list())
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

ratings_results <- evaluate(Multi_evaluate, algorithms, type = "ratings")

plot(ratings_results)

```

In the previous plots we can see the best results come from the *POPULARITY* algorithm.

In the ROC curve, we find a steep increase in True Positive Rates until the tenth recommendation. We would not recommend more than 10 movies considering a possible bigger marginal increment in False Positive Rates.

It is important to consider that the POPULARITY algorithm is based in the most popular items. From a business perspective, this may affect negatively the recommendations incurring in the banana problem (Only recommends top rating items). Considering this, we would create a Hybrid Recommendation Engine that includes *Popularity* and *User Based* Recommendations with a small number of nearest neighbors.

## Exploring the UBCF Parameters

If we increase the number of Nearest Neighbors in the UBCF algorithm, we see that the AUC improves, with a tendency towards the Popularity algorithm. This happens because we are considering a larger number of neighbors tending towards the usage of the complete users in the dataset. From a business perspective we would like to have a balance in the recommendations of popular items with personalized items, considering only what the most similar reduced set of neighbors prefer. This may drop the accuracy level of the algorithm but consider more recommendations that will impact positively the business.

```{r}


algorithms2 <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn10" = list(name="UBCF", param=list(nn=10)),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFnn100" = list(name="UBCF", param=list(nn=100)),
  "User Based CFnn200" = list(name="UBCF", param=list(nn=200))
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms2, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

```

# Creating a Recomendation Engine

Our hybrid recommendation engine proposal is more weighted towards the User Based Collaborative Filtering (70%) than towards the popularity (30%). We want to give more importance to recommend a broader set of movies suited to user preferences. 

```{r}
MoviesTrainDS <- bigMovieLense[1:(nrow(bigMovieLense)*0.8),]
MovieTestDS <- bigMovieLense[4833:nrow(bigMovieLense),]

#Recommender of type POPULAR 
Popular_recMovies <- Recommender(MoviesTrainDS, method="POPULAR")
# Recommender UBCF
UBCF_recMovies <- Recommender(MoviesTrainDS, method = "UBCF", parameter = list(nn=10))
# Hybrid Recommender
hybrid_rec <- HybridRecommender(Popular_recMovies, UBCF_recMovies, weights = c(0.3,0.7))
```

### Recommending Movies

Now that we created a Hybrid recommender merging Popular and User Based CF, we will compare the results of the 10 recommended movies: 

* The first list displays the Hybrid Recommender 30% Popularity and 70% UBCF.
* The second list displays a 100% Popularity-based recommender.
* The third list displays a 100% UBCF with 10 nearest neighbors.

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict <- predict(hybrid_rec, newdata = MovieTestDS[1,], n=10)
as(hybrid_predict, "list")

Popular_predict <- predict(Popular_recMovies, newdata = MovieTestDS[1,], n=10)
as(Popular_predict, "list")

UBCF_predict <- predict(UBCF_recMovies, newdata = MovieTestDS[1,], n=10)
as(UBCF_predict, "list")

```

We can see that inside the 10 recommended movies of our Hybrid Algorithm we find some of the Highest Viewed / Highest Rated movies such as American Beauty but also movies more suited to the user's preferences. This also happens to another randomly selected user:

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict2 <- predict(hybrid_rec, newdata = MovieTestDS[200,], n=10)
as(hybrid_predict2, "list")

Popular_predict2 <- predict(Popular_recMovies, newdata = MovieTestDS[200,], n=10)
as(Popular_predict2, "list")

UBCF_predict2 <- predict(UBCF_recMovies, newdata = MovieTestDS[200,], n=10)
as(UBCF_predict2, "list")

```
