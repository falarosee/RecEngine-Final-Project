---
title: "Recommendation Engines Final Project"
author: "Franz-Anton La Rosee"
date: "01/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}

# Load/Install packages

if(!"dplyr" %in% installed.packages()) {install.packages("dplyr")}
library(dplyr)

if(!"class" %in% installed.packages()) {install.packages("class")}
library(class)

if(!"recommenderlab" %in% installed.packages()) {install.packages("recommenderlab")}
library(recommenderlab)

if(!"shiny" %in% installed.packages()) {install.packages("shiny")}
library(shiny)
```

# 1. Executive Summary:

## 1.1 Project Aim & Introduction

This markdown contains the prototype and development of two recommendation engines, where one provides recommendations for groceries based on past transactions and the other provides recommendations for movies, based on user ratings. The final models were selected following the CRISP-DM framework for each business case: Business Understanding -> Data Understanding -> Data Preperation -> Modelling -> Model Evaluation, where special attention was given to the nature of each business problem for the model selection, prior to any performance evaluations. 

Given the seperate nature, aim and construction of each recommendation engine, this markdown is split into two parts, where section 2. contains the technical methodology for the development of the groceries recommendation engine and section 3. contains the technical methodology and development for the movies recommendation engine.

The prototypes of the final models are stated in the following two sub-sections below. A detailed methodology of how these models were selected can be found in sections (2.4) for grocery recommendations and (3.4) for movie recommendations.

## 1.2 Business Case & Prototype for Grocery Recommendations

The business objective of the recommendation engine for groceries is to increase sales of the online groceries store of the client, a major spanish supermarket chain. An association rule model prototype was constructed to provide recommendations for groceries based on past customer transactions. Recommendations are based on computing rules that capture whether the purchase of one, or a group of grocery itmes purchased within a single transaction, increase the probability of purchasing another grocery item.

Within the deployment phase, the model is to be embedded within the online store and become part of the online shopping experience. Grocery item recommendations are made based on association rules that meet items that were already placed inside the shopping cart by a customer. Recommended items can be displayed via unintrusive pop-ups that appear when a rule is met that passes a threshold that is to be set in accordance with the clients' business managers. 

Deployment of the model will overtime improve recommendations due to an implicit feedback loop, where new rules are created based on customer transactions and existing rules are strengthened by transactions that were affected by them. The latter happens when a customer accepts a recommendation based on a rule and proceeds to check out. This results in a further transaction that is added to the data which corresponds to-, and thereby reinforces the rule.

Thought can be given to implementing an explicit feedback loop, where the customer is given an option to rate a recommendation with a thumbs up or thumbs down. Alternatively, the direct acceptance of a recommendation can be considered as positive explicit feedback. These explicit ratings can be added as a further weight in the ranking of the recommended items.  

The prototype of this model, along with the 20 most powerful rules is displayed below.  Thought was given to decide which association metrics (support, confidence, rule support, lift, confidence difference or confidence ratio) should be the decisive factor in setting the threshold for the deployment. According to best practices outlined by Lallich Et Al. (2005)[^1], lift, confidence difference and confidence ratio were deemed the most appropriate in comparing meaningful transaction driving association rules. Therefore, appropriate thresholds can be set for each metric in accordance with business experience during the deployment phase. For an explanation of the meaning of each metric, see **Appendix A**.

[^1]https://pdfs.semanticscholar.org/1671/eee808d8c8dfdb7a704962ca90b579cb8d1d.pdf 

### Groceries Recommendation Model Prototype

The model prototype, along with the 20 most powerful rules according to lift, confidence difference or confidence ratio can be explored below:


```{r Groceries Prototype}

groceriesRecAr <- Recommender(groceriesDataset, method = "AR", parameter = list(support = 0.001, conf = 0.001, maxlen = 70))

arGroceriesModel <- getModel(groceriesRecAr)$rule_base
rulesDF <- as(arGroceriesModel, "data.frame")
rulesDF$prior_conf <- rulesDF$confidence / rulesDF$lift
rulesDF$confidence_diff <- abs(rulesDF$confidence - rulesDF$prior_conf)
rulesDF$confidence_ratio <- 1- (apply(rulesDF[,c(3,7)], 1, min) / apply(rulesDF[,c(3,7)], 1, max) )

# Top 20 according to each AR metric
lift<-head(rulesDF[order(rulesDF$lift, decreasing = T),],20)
conf_rat<-head(rulesDF[order(rulesDF$confidence_ratio, decreasing = T),],20)
conf_dif<-head(rulesDF[order(rulesDF$confidence_diff, decreasing = T),],20) # Rules with longer LHS usually have higher confidence_difference
```

##### Most Popular Rules according to Lift
``` {r Lift Rules}
print(lift$rules)
```
##### Most Popular Rules according to Confidence Difference

``` {r Lift Rules}
print(conf_dif$rules)
```

``` {r Lift Rules}
print(conf_rat$rules)
```

For a detailed methodology of the model construction, see section 2.

## 1.3 Business Case and Prototype for Movie Recommendations

The business objective of the recommendation engine for movies is to increase customer streaming time and improve customer experience of a the client, a major spanish movie streaming platoform. A hybrid model prototype, consisting of collaborative filtering, random generator and popularity elements has been developed to provide user-centric recommendations for movie suggestions for the user to stream. Recommendations are generated based on past user ratings.  

Within the deployment phase, the model is to be embedded within the streaming platform and become part of the online streaming experience. Movie recommendations are made based on users with similar tastes (collaborative filtering), overall popularity and an element of randomness. Recommended movies can be displayed via a dedicated section on the user homepage and filtered according to dimensions such as genre and year.  

Deployment of the model can overtime improve recommendations by implicit and explicit feedback loops. Implicit feedback is extracted through the collaborative filtering element of the hybrid model, where recommendations based on collaborative filtering are reinforced by users who follow recommendations by watching recommended movies. To avoid a feedback loop that continously narrows the recommendations scope, by recommending similar movies that get re-enforced by user behaviour and consequently recommend more of the same, an element of randomness is built into the model that occasionally recommends movies to the user that are outside of the user's usual viewing behaviour.

It is recommended to implement an explicit feedback loop, where the customer is given an option to rate a recommended movie with a thumbs up or thumbs down, after having viewed it. This explicit user feedback adds further information that helps to avoid the possibility of positive implicit feedback for movies that the user watched based on a recommendation, but didn't end up liking. The explicit feedback is therefore an important tool to correct a possible compounding of errors, caused by recommended movies the user watched and did not like. 

The prototype of this model, along with its most significant performance metrics is displayed below. Thought was given to setting apporpriat waits for the different elements of the hybrid, as well as setting an appropriate number of nearest neighbours. As users are not expected to watch many movies within a short time frame, the number of nearest neighbours was kept small. This enables the recommendation engine to start recommending without requiring a large number of watched movies for each user. The elements of the hybrid have been weighted according to their importance, where collaborative filtering is considered most important to tailor customized results (0.6), overall popularity is included to capture quick-win, popular movies and randomness (0.1) is kept small in order prevent scope narrowing, without significantly affecting recommendation quality.

### Movies Recommendation Model Prototype

# Creating a Recomendation Engine

Our hybrid recommendation engine proposal is more weighted towards the User Based Collaborative Filtering (70%) than towards the popularity (30%). We want to give more importance to recommend a broader set of movies suited to user preferences. 

```{r}
MoviesTrainDS <- bigMovieLense[1:(nrow(bigMovieLense)*0.8),]
MovieTestDS <- bigMovieLense[4833:nrow(bigMovieLense),]

#Recommender of type POPULAR 
Popular_recMovies <- Recommender(MoviesTrainDS, method="POPULAR")
# Recommender UBCF
UBCF_recMovies <- Recommender(MoviesTrainDS, method = "UBCF", parameter = list(nn=10))
# Random Recommender
Random_recMovies <- Recommender(MoviesTrainDS, method = 'RANDOM')
# Hybrid Recommender
hybrid_rec <- HybridRecommender(Popular_recMovies, UBCF_recMovies, Random_recMovies ,weights = c(0.3,0.6,0.1))

```

### Recommending Movies

Now that we created a Hybrid recommender merging Popular and User Based CF, we will compare the results of the 10 recommended movies: 



```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict <- predict(hybrid_rec, newdata = MovieTestDS[1,], n=10)
as(hybrid_predict, "list")
```

# 2. Recommendation Engine For Groceries: Technical Methodology

## 2.1 Data Exploration

The dataset provided for analysis contained grocery purchases pertaining to 9835 transactions and 169 grocery items, where each row represents a transaction and each column represents an item. It was cleaned and ready for analysis upon receipt. The eighteen 169 variables were identified as binaries, as they have values of 0 and 1 to classify whether the item was purchased (1) or was not purchased (0) within the transaction represented by the row. 

Several different associative metrics and thresholds were tested during data analysis. Those identified as most relevant to the objective (Lift, Confidence ratio and Confidence difference) were selected for inclusion into the model for final recommendations. These metrics were identified according to best practices1 and deemed as the most reliable and effective way of measuring the importance of rules generated, in terms of increasing overall course sales. The results from the selected metrics are presented below. For an explanation of the meaning of each metric, see Appendix B.

Load the groceries dataset, and transform it to a binaryRatingMatrix, from the recommenderlab package.

```{r load data}
data("Groceries")

```

```{r exploratory}
groceries_summary <- summary(Groceries)
groceries_summary
plot(groceries_summary@lengths)
itemFrequencyPlot(Groceries, topN = 20)

```


The Groceries dataset consists of 169 columns and 9835 rows, that respectively represent the different items and the different transactions.
The dataset represents different transactions (baskets). 

#so the recommendation algorithm that comes to mind here is Association Rules.

With the help of some plots, we explore the groceries dataset.
First of all, we plot the distribution of the amount of items in every transaction. As we can see on the plot, a lot of the transactions contain exactly 1 item (2159 transactions). The number of transactions with x items decreases as x increases. On average, every transaction contains 4.409 unique items.

A random sample of 100 transactions for the groceries dataset confirms the sparsity of the dataset: only a few items are bought in every transaction and the majority of the items occure in a small part of the transactions. 

```{r Sparsity}

image(sample(Groceries, 100))

```

## 2.2 Data Preperation

The groceries dataset is actually a binary matrix. This matrix doesn't contain any real ratings, so normalization is not possible in this case. We just convert the transactions 'Groceries' to a binaryRatingMatrix 'groceriesDataset'. With this binaryRatingMatrix we can build our recommenders.

```{r Convert ot Binary matrix}

groceriesDataset <- as(Groceries, "binaryRatingMatrix")

```

## 2.3 Model Selection

The following output shows all the possible algorithms for a binaryRatingMatrix.

```{r Show all the possible algorithms for a binaryRatingMatrix, echo=TRUE}

recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")

```

There are six possible recommendation algorithms for the groceries dataset, namely: Alternative Least Squares (ALS), Association Rules (AR), Item Based Collaborative Filtering (IBCF), Popular, Random and User Based Collaborative Filtering (UBCF).
We are going to take a look at the AR, IBCF, UBCF and Popular algorithms, and we are going to use the Random algorithm as a baseline.

#---STATISTICAL EVALUATION---#

First we are going to compare these algorithms according to their ROC curve. We split the data in train and test data. Then we evaluate their performance with a model that was trained on the train set (one model for every algorithm). The model then returns a list with N recommendations for every transaction in the test set. For N, we choose the values 1, 5, 10, 15 and 20 because ultimately it doesn't make sense to recommend more than 20 grocery store items. For the 'given' parameter in the evaluationScheme function we chose -1, that way the algorithms get all but one items from a transaction and they try to predict the one extra item in that transaction. This makes sense because there are not that many items in each transaction, so it's easier to make a prediction for one item, on the basis of the rest of the transaction. From a business perspective this also makes sense because it is usually the case that a recommendation is done, according to the items that are already in your basket (in online shopping). 

```{r Split the data}

# 80-20 for train and test set
esSplitGroc <- evaluationScheme(groceriesDataset, method="split", train=0.80, given = -1)

```

#---TUNING UBCF---#

For the parameter tuning of the different algorithms, we only need to look at the UBCF, IBCF and the AR algorithms because the popular and random ones don't need any parameters.
The UBCF algorithm needs an 'nn' parameter, which specifies the number of nearest neighbours that the algorithm should look at. We will make several UBCF algorithms with the parameter ranging from 100 to 1000, with a step of 100 to see which parameter value yields the best results. 
In this case, for groceries, there are no actual users, because the rows in the groceries matrix represent transactions and not users. So User Based CF is actually Transaction Based CF in this case. It looks for similar transactions and then recommends items that were present in these similar transactions.

```{r Parameter tuning UBCF}

vector_nn <- seq(100,1000,100)
models_to_evaluate <- lapply(vector_nn, function(k){
  list(name = "UBCF",
       param = list(nn = k))
})
names(models_to_evaluate) <- paste0("UBCF_nn", vector_nn)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsIBCF, annotate=c(1,2))

```

As we can see on the graph, the algorithm with nn = 400 performs the best, although there is not much difference between the algorithms. 

#---TUNING IBCF---#

For IBCF, we can specify two parameters: the distance method to use and the parameter k, which is comparable to the parameter nn of UBCF and also specifies the neighbourhood for comparison. 
The parameter k will vary between 100 and 250 in this case, with steps of 50, and we will compare the method 'Jaccard' and 'Cosine'.

```{r Parameter tuning IBCF}

vector_k <- c(100,150,200,250)
models_to_evaluateJac <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "Jaccard", k = k))
})
names(models_to_evaluateJac) <- paste0("IBCF_k_Jac", vector_k)

models_to_evaluateCos <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "cosine", k = k))
})
names(models_to_evaluateCos) <- paste0("IBCF_k_Cos", vector_k)

models_to_evaluate <- append(models_to_evaluateJac, models_to_evaluateCos)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 20))


plot(resultsIBCF, annotate=c(1,2))

```

For IBCF, we see that the Jaccard method works best, especially when less recommendations are made. In this case, it is much harder to detect the beter parameter because they are all really comparabl. We will choose 150 as the best parameter, although, again, the performance of the algorithms are very close for the different parameters.

#---COMPARISON ALL---#
Now we compare all these algorithms, with their optimal parameters, with each other. For AR we choose our parameters in such a way that enough rules are generated to make enough recommendations so we can evaluate it easily with the other algorithms.

```{r Make the models}

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "UBCF" = list(name="UBCF", param=list(nn=400)),
  "IBCFJac" = list(name="IBCF", param=list(method = "Jaccard",
                                           k=150)),
  "AR" = list(name="AR", param = list(support = 0.001, conf = 0.01, maxlen=20))
)

resultsGroc <- evaluate(esSplitGroc, algorithms, type = "topNList", n=c(1, 5, 10, 15, 20))

plot(resultsGroc, annotate=c(1), legend = "topright")

```

This plot shows that the UBCF algorithm gives the best results. The rest of the algorithms actually yield similar results. For UBCF and IBCF, it is the case that for a growing parameter nn and k respectively the algorithms get similar to the 'popular' algorithm because it takes more neighbours into account and ultimately it will take every User/Item into account, just like the 'popular' algorithm does.
The UBCF algorithm yields the best results, however from a business perspective it is not the most interesting algorithm. Mainly because it doesn't really give an insight in how several items are related, it just makes recommendations based on items that are similar. It is of more use to us to know the items that get frequently bought together and have some kind of rules that explain these relationships between items.
Because the difference between the algorithms isn't that big, we choose to advance with the AR algorithm because it makes the most sense in this business case and it is easier to draw conclusions from these rules. ###--ADD MORE REASONS DISCUSSED--##

## 2.4 Final Model Tuning

In this section, we are going to tune the parameter for our AR algorithm.

Support -> really small, because when higher, items with high frequency are put into every rule (same items all the time - bannana problem)

conf -> should be high 

higher constraints = less rules

```{r check AR rules plaything}

groceriesRecAr2 <- Recommender(groceriesDataset, method = "AR", parameter = list(support = 0.001, conf = 0.001, maxlen = 50))

arGroceriesModel <- getModel(groceriesRecAr2)$rule_base
summary(arGroceriesModel)
rulesDF <- as(arGroceriesModel, "data.frame")
rulesDF$prior_conf <- rulesDF$confidence / rulesDF$lift
rulesDF$confidence_diff <- abs(rulesDF$confidence - rulesDF$prior_conf)
rulesDF$confidence_ratio <- 1- (apply(rulesDF[,c(3,7)], 1, min) / apply(rulesDF[,c(3,7)], 1, max) )

# Top 20 according to each AR metric
head(rulesDF[order(rulesDF$lift, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_ratio, decreasing = T),],20)
head(rulesDF[order(rulesDF$confidence_diff, decreasing = T),],20) # Rules with longer LHS usually have higher confidence_difference
head(rulesDF[order(rulesDF$confidence, decreasing = T),],20) 
head(rulesDF[order(rulesDF$support, decreasing = T),],20)
```


# 3. Recommendation Engine For Movies: Technical Methodology

```{r}
load("bigMovieLense.RData")
```

The following document shows the analysis of different recommendation algorithms applied to the _Big Movie Lense dataset_. The main objective is to apply variations of recommendation algorithms to understand which performs better and why.

As we can see, the dataset is a rating matrix with 999,714 ratings from 6,040 users to 3,449 movies. The ratings span is from 1 to 5.

```{r}
str(bigMovieLense)
```

# Descriptive analysis

As an initial step, we will analyze the distribution of Ratings.
We can see that on average one user has evaluated 166 movies.
The user that has the minimum number of evaluations has only 18, while the user with the maximum number has 2,283.

Considering that some users tend to give low or high ratings to all of their movies, the results may be biased. To remove this effect, the data must be normalized in such a way that the average rating of each user is 0. 

The following histograms show the distribution of ratings from users. We can see that the ratings are slightly skewed to the left, indicating that most of the movies are rated with 4/5.


```{r}

# On average wow many evaluations per user we have?
mean(rowCounts(bigMovieLense))
min(rowCounts(bigMovieLense))
max(rowCounts(bigMovieLense))

# Which are the evaluations from the first user?
#as(bigMovieLense[1,], "list")

# On average, how much the first user rates?
rowMeans(bigMovieLense[1,])

# Distribution of the ratings
hist(getRatings(bigMovieLense), col = "darkred")
hist(getRatings(normalize(bigMovieLense, method = "Z-score")), main="Z-score Ratings Distribution", col = "Red")

#distribution of how many movies each user has rated
hist(rowCounts(bigMovieLense), breaks = 50, main = "Histogram Rated movies by user", col = "lightBlue")

# Distribution of the average ratings per movie
hist(colMeans(bigMovieLense), main = "Histogram Average Rating per Movie", col = "lightGreen")
```

In the following Rating Matrix Heat map we subset the first 50 users and 100 movies. We can see that the matrix is highly sparsed and some users tend to evaluate in a more aggressive/non-aggressive way. This supports the fact that the information has to be normalized for a correct analysis. 

```{r}

image(getRatingMatrix(bigMovieLense[1:50,1:100]), main="Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

image(getRatingMatrix(normalize(bigMovieLense, method="Z-score")[1:50,1:100]), main=" Normalized Rating Matrix Heatmap", xlab = "Movies", ylab="Users")

```

Now we will analyze which are the most viewed movies (based in the number of ratings) and the top average rated. We can see there is no clear relation between total views and top rated.

```{r}

views_per_movie <- colCounts(bigMovieLense)
table_view <- data.frame(movie = names(views_per_movie), views = views_per_movie)
table_view <- table_view[order(table_view$views, decreasing = T),]

avg_rating_pre_movie <- colMeans(bigMovieLense)
top_rated <- data.frame(movie = names(avg_rating_pre_movie), rating = avg_rating_pre_movie)
top_rated <- top_rated[order(top_rated$rating, decreasing = T),]

avg_movies <- merge(table_view, top_rated, by = "movie")
top_views <- avg_movies[order(avg_movies$views, decreasing = T),]
top_rated <- avg_movies[order(avg_movies$rating, decreasing =T),]

# Top viewed movies
print(as(top_views[1:10,],"matrix"))

# Top rated movies
print(as(top_rated[1:10,],"matrix"))

plot(avg_movies$rating, avg_movies$views, main = "Ratings vs Views", xlab="Movie Average Rating", ylab = "Total Views", col="lightblue")

```

# Comparing Recommendation Algorithms

### Evaluation Scheme

We create an evaluation scheme with 80% training set and 20% testing. 
For the test, 5 items will be given to the recommender algorithm and the others will be held out for computing the error. In this case having a small "given" parameter makes sense because the movie consumption is not as frequent as for example songs. We would only need five movie ratings by the user to start recomending. 

Considering that the ratings are distributed from 1 to 5, we assume a good rating of 4 and 5. This binarization metric is important for understandig the popularity of an item. 

We may perform a K-fold cross evaluation to better compute the algorithm performance indicators. However, to optimize the running time, a single split is taken into account.

### Algorithms

We will test recommendation systems with the following algoriths:

* *Random* --> Produce random recommendations. This algorithm serves as a benchmark to confirm that the algorithms give a better prediction than a random recommender.
* *Popularity* --> Recomendation based in items popularity.

* *UBCF* --> The user-based CF assumes that similar users tend to give similar rates. There are different similarity measures between users. For this exercise we will test the Nearest Neighbors (nn=50) and the Cosine similarity measure.

** User Based Collaborative Filtering (UBCF) NN = 50 --> The recommendations are given based in the 50 nearest neighbors.
** User Based Collaborative Filtering (UBCF) Cosine --> The recommendations are given based in the cosine similarity measure. 

* *IBCF Pearson* --> The item-based CF recommends items based on their similarity with the items that the target user rated. The advantage of this algorithm family is that it tackles the issue that users' prefrences may change over time. For this exercise we test the Pearson similarity measure. We can also use the Cosine or Jaccard similarity measures. However, the Pearson correlation is invariant to adding a constant to all elements.

* *Alternating Least Squares Matrix Factorization (ALS)* 
* *Singular Value Decomposition (SVD)*

In collaborative filtering, the scalability issue is common with large datasets. If we add more users and movies, the matrix grows exponentially, and the majority of movies receive few or even no ratings ending with a really sparse matrix. The ALS and SVD algorithms are matrix factorization solutions for collaborative filtering, when we need to scale the recommendation system. We will test the ALS and SVD algorithms assuming that we would require scalability in the future.


```{r}

# Get the different Algorithms that we may apply to our Recommendation Engine
recommenderRegistry$get_entries(dataType = "realRatingMatrix")

MoviesSubset <- bigMovieLense[1:(nrow(bigMovieLense)*0.4),]

set.seed(123)
Multi_evaluate <- evaluationScheme(MoviesSubset, method = "split", train=0.8, given = 5, goodRating = 4)

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFCos" = list(name="UBCF", param=list(method = "Cosine")),  
  "Item Based CFPea" = list(name="IBCF", param=list(method = "Pearson")),
  "ALS" = list(name="ALS", param=list(k=50)),
  "SVD" = list(name="SVD", param=list(k=50))
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

ratings_results <- evaluate(Multi_evaluate, algorithms, type = "ratings")

plot(ratings_results)

```

In the previous plots we can see the best results come from the *POPULARITY* algorithm.

In the ROC curve, we find a steep increase in True Positive Rates until the tenth recommendation. We would not recommend more than 10 movies considering a possible bigger marginal increment in False Positive Rates.

It is important to consider that the POPULARITY algorithm is based in the most popular items. From a business perspective, this may affect negatively the recommendations incurring in the banana problem (Only recommends top rating items). Considering this, we would create a Hybrid Recommendation Engine that includes *Popularity* and *User Based* Recommendations with a small number of nearest neighbors.

## Exploring the UBCF Parameters

If we increase the number of Nearest Neighbors in the UBCF algorithm, we see that the AUC improves, with a tendency towards the Popularity algorithm. This happens because we are considering a larger number of neighbors tending towards the usage of the complete users in the dataset. From a business perspective we would like to have a balance in the recommendations of popular items with personalized items, considering only what the most similar reduced set of neighbors prefer. This may drop the accuracy level of the algorithm but consider more recommendations that will impact positively the business.

```{r}


algorithms2 <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn10" = list(name="UBCF", param=list(nn=10)),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFnn100" = list(name="UBCF", param=list(nn=100)),
  "User Based CFnn200" = list(name="UBCF", param=list(nn=200))
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms2, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

```

# Creating a Recomendation Engine

Our hybrid recommendation engine proposal is more weighted towards the User Based Collaborative Filtering (70%) than towards the popularity (30%). We want to give more importance to recommend a broader set of movies suited to user preferences. 

```{r}
MoviesTrainDS <- bigMovieLense[1:(nrow(bigMovieLense)*0.8),]
MovieTestDS <- bigMovieLense[4833:nrow(bigMovieLense),]

#Recommender of type POPULAR 
Popular_recMovies <- Recommender(MoviesTrainDS, method="POPULAR")
# Recommender UBCF
UBCF_recMovies <- Recommender(MoviesTrainDS, method = "UBCF", parameter = list(nn=10))
# Random Recommender
Random_recMovies <- Recommender(MoviesTrainDS, method = 'RANDOM')
# Hybrid Recommender
hybrid_rec <- HybridRecommender(Popular_recMovies, UBCF_recMovies, Random_recMovies ,weights = c(0.3,0.6,0.1))

```

### Recommending Movies

Now that we created a Hybrid recommender merging Popular and User Based CF, we will compare the results of the 10 recommended movies: 

* The first list displays the Hybrid Recommender 30% Popularity and 70% UBCF.
* The second list displays a 100% Popularity-based recommender.
* The third list displays a 100% UBCF with 10 nearest neighbors.

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict <- predict(hybrid_rec, newdata = MovieTestDS[1,], n=10)
as(hybrid_predict, "list")

Popular_predict <- predict(Popular_recMovies, newdata = MovieTestDS[1,], n=10)
as(Popular_predict, "list")

UBCF_predict <- predict(UBCF_recMovies, newdata = MovieTestDS[1,], n=10)
as(UBCF_predict, "list")
```

We can see that inside the 10 recommended movies of our Hybrid Algorithm we find some of the Highest Viewed / Highest Rated movies such as American Beauty but also movies more suited to the user's preferences. This also happens to another randomly selected user:

```{r}
# Top-10 recommendation list for the first user of the test set

hybrid_predict2 <- predict(hybrid_rec, newdata = MovieTestDS[200,], n=10)
as(hybrid_predict2, "list")

Popular_predict2 <- predict(Popular_recMovies, newdata = MovieTestDS[200,], n=10)
as(Popular_predict2, "list")

UBCF_predict2 <- predict(UBCF_recMovies, newdata = MovieTestDS[200,], n=10)
as(UBCF_predict2, "list")

```
