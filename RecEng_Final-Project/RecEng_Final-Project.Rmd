---
title: "Recommendation Engines Final Project"
author: "Franz-Anton La Rosee"
date: "01/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}

# Load/Install packages

if(!"dplyr" %in% installed.packages()) {install.packages("dplyr")}
library(dplyr)

if(!"class" %in% installed.packages()) {install.packages("class")}
library(class)

if(!"recommenderlab" %in% installed.packages()) {install.packages("recommenderlab")}
library(recommenderlab)

if(!"shiny" %in% installed.packages()) {install.packages("shiny")}
library(shiny)
```

# 1. Executive Summary:

## 1.1 Project Aim & Model Results

This markdown contains the development of two recommendation engines, where one provides recommendations for groceries based on past transactions and the other provides recommendations for movies, based on user ratings. The final models were selected following the CRISP-DM framework for each business case: Business Understanding -> Data Understanding -> Data Preperation -> Modelling -> Model Evaluation, where special attention was given to the nature of each business problem for the model selection, prior to any performance evaluations. 

According to this approach, the following two models were selected, with their corresponding results:

### Final Model - Recommendation Engine For Groceries

To provide recommendations for groceries based on past transactions an association analysis was performed to identify rules that define whether the purchase of one or more groceries would increase the probability of a purchase of another grocery item. To see the methodology of how this model was selected, see Section 2.4, model selection. 

Amongst the many rules discovered, BLANK distinct rules have been identified in terms of statistical validity and value for business. 

Insights into purchasing patterns gained from these rules are intended to guide the business manager to increase grocery sales, by discovering item groups that customers tend to buy together.The analysis herein utilized sales information contained in the dataset “Groceries”. 
By utilizing the associative metrics Lift, Confidence difference and Confidence ratio, the following set of BLANK rules was identified:


### Model Results - Recommendation Engine For Movies

Given the seperate nature, aim and construction of each recommendation engine, the model development section of this markdown is split into two parts, where section 2.1 contains the development for the groceries recommendation engine and section 2.2 contains the development for the movies recommendation engine. 

# 2. Model Development: Recommendation Engine For Groceries

## 2.1 Business Case

## 2.2 Data Exploration

Load the groceries dataset, and transform it to a binaryRatingMatrix, from the recommenderlab package.

```{r load data}
data("Groceries")
groceriesDataset <- as(Groceries, "binaryRatingMatrix")

#str(groceriesDataset)

```

The Groceries dataset consists of 169 columns and 9835 rows, that respectively represent the different items and the different transactions for each customer.
The dataset represents different transactions (baskets), so the recommendation algorithm that comes to mind here is Association Rules.

With the help of some plots, we explore the groceries dataset.
First of all, we plot the distribution of the amount of items in every transaction. As we can see on the plot, most of the transactions contain exactly 1 item (2159 transactions). The number of transactions with x items decreases as x increases. On average, every transaction contains 4.409 unique items.

```{r exploratory}
groceries_summary <- summary(Groceries)
groceries_summary
plot(groceries_summary@lengths)
itemFrequencyPlot(Groceries, topN = 20)

```
## 2.3 Data Preperation

## 2.4 Model Selection

The following output shows all the possible algorithms for a binaryRatingMatrix.

```{r Show all the possible algorithms for a binaryRatingMatrix}

recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")

```

There are six possible recommendation algorithms for the groceries dataset, namely: Alternative Least Squares (ALS), Association Rules (AR), Item Based Collaborative Filtering (IBCF), Popular, Random and User Based Collaborative Filtering (UBCF).
We are going to take a look at the AR, IBCF, UBCF and Popular algorithms, and we are going to use the Random algorithm as a baseline.

```{r Make the train and test set, and model}

# 80-20 for train and test set
esSplitGroc <- evaluationScheme(groceriesDataset, method="split", train=0.80, given = -1)

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "UBCF" = list(name="UBCF", param=list(nn=500)),
  "IBCFJac" = list(name="IBCF", param=list(method = "Jaccard",
                                           k=500)),
  "AR" = list(name="AR", param = list(support = 0.001, conf = 0.3, maxlen=5))
)

resultsGroc <- evaluate(esSplitGroc, algorithms, type = "topNList", n=c(1, 5, 10, 15, 50, 100))

plot(resultsGroc, annotate=c(1,2))

```

```{r Get best parameter}

vector_k <- c(100,150,169,300,350,400)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "Jaccard", k = k))
})
names(models_to_evaluate) <- paste0("IBCF_k_", vector_k)

resultsIBCF <- evaluate(esSplitGroc, models_to_evaluate, type = "topNList", n=c(1, 5, 10, 15, 50, 100))


plot(resultsIBCF, annotate=c(1,2))

```


## 2.5 Model Tuning

This is just some code to test and see the AR rules

```{r check AR rules}

groceriesRecAr <- Recommender(getData(esSplitGroc,"train"), method = "AR", parameter = list(support = 0.001, conf = 0.2, maxlen = 5))



arGroceriesModel <- getModel(groceriesRecAr)$rule_base
summary(arGroceriesModel)
rulesDF <- as(arGroceriesModel, "data.frame")
rulesDF$prior_conf <- rulesDF$confidence / rulesDF$lift
rulesDF$confidence_diff <- abs(rulesDF$confidence - rulesDF$prior_conf)
rulesDF$confidence_ratio <- 1- (apply(rulesDF[,c(3,7)], 1, min) / apply(rulesDF[,c(3,7)], 1, max) )

rulesDF[order(rulesDF$lift, decreasing = T),]
rulesDF[order(rulesDF$confidence_ratio, decreasing = T),]
rulesDF[order(rulesDF$confidence_diff, decreasing = T),]
rulesDF[order(rulesDF$confidence, decreasing = T),]
rulesDF$

inspect(head(sort(arGroceriesModel, by="lift", decreasing=TRUE), 20))
inspect(head(sort(arGroceriesModel, by="confidence", decreasing=TRUE), 20))
inspect(head(sort(arGroceriesModel, by="support", decreasing=TRUE), 20))


```


# 3. Model Development: Recommendation Engine For Movies

```{r}
load("bigMovieLense.RData")
```

The following document shows the analysis of different recommendation algorithms applied to the _Big Movie Lense dataset_. The main objective is to apply variations of recommendation algorithms to understand which performs better and why.

As we can see, the dataset is a rating matrix with 999,714 ratings from 6,040 users to 3,449 movies. The ratings span is from 1 to 5.

```{r}
str(bigMovieLense)
```

# Descriptive analysis

As an initial step, we will analyze the distribution of Ratings.
We can see that on average one user has evaluated 166 movies.
The user that has the minimum number of evaluations has only 18, while the user with the maximum number has 2,283.

Considering that some users tend to give low or high ratings to all of their movies, the results may be biased. To remove this effect, the data must be normalized in such a way that the average rating of each user is 0. 

The following histograms show the distribution of ratings from users. We can see that the ratings are slightly skewed to the left, indicating that most of the movies are rated with 4/5.


```{r}

# On average wow many evaluations per user we have?
mean(rowCounts(bigMovieLense))
min(rowCounts(bigMovieLense))
max(rowCounts(bigMovieLense))

# Which are the evaluations from the first user?
#as(bigMovieLense[1,], "list")

# On average, how much the first user rates?
rowMeans(bigMovieLense[1,])

# Distribution of the ratings
hist(getRatings(bigMovieLense), col = "darkred")
hist(getRatings(normalize(bigMovieLense, method = "Z-score")), main="Z-score Ratings Distribution", col = "DarkRed")

#distribution of how many movies each user has rated
hist(rowCounts(bigMovieLense), breaks = 50, main = "Histogram Rated movies by user", col = "DarkBlue")

# Distribution of the average ratings per movie
hist(colMeans(bigMovieLense), main = "Histogram Average Rating per Movie", col = "DarkGreen")

```

# Comparing Recommendation Algorithms

We will test recommendation systems with the following algoriths:

* *Random* --> Produce random recommendations. This algorithm serves as a benchmark to confirm that the algorithms give a better prediction than a random recommender.
* *Popularity* --> Recomendation based in items popularity.

* *UBCF* --> The user-based CF assumes that similar users tend to give similar rates. There are different similarity measures between users. For this exercise we will test the Nearest Neighbors (nn=50) and the Cosine similarity measure.

** User Based Collaborative Filtering (UBCF) NN = 50 --> The recommendations are given based in the 50 nearest neighbors.
** User Based Collaborative Filtering (UBCF) Cosine --> The recommendations are given based in the cosine similarity measure.

* *IBCF Pearson* --> The item-based CF recommends items based on their similarity with the items that the target user rated. The advantage of this algorithm family is that it tackles the issue that users' prefrences may change over time. For this exercise we test the Pearson similarity measure. We can also use the Cosine or Jaccard similarity measures. However, the Pearson correlation is invariant to adding a constant to all elements.

* *Alternating Least Squares Matrix Factorization (ALS)* --> In collaborative filtering, the scalability issue is common with large datasets. If we add more users and movies, the matrix grows exponentially, and the majority of movies receive few or even no ratings ending with a really sparse matrix. The ALS algorithm is a matrix factorization solution for collaborative filtering, when we need to scale the recommendation system. We will test the ALS algorithms assuming that we would require scalability in the future.

```{r}

# Get the different Algorithms that we may apply to our Recommendation Engine
recommenderRegistry$get_entries(dataType = "realRatingMatrix")

MoviesSubset <- bigMovieLense[1:(nrow(bigMovieLense)*0.50),]

set.seed(123)
Multi_evaluate <- evaluationScheme(MoviesSubset, method = "split", train=0.8, given = 5, goodRating = 4)

algorithms <- list(
  "random" = list(name="RANDOM", param=NULL),
  "popular" = list(name="POPULAR", param=NULL),
  "User Based CFnn50" = list(name="UBCF", param=list(nn=50)),
  "User Based CFCos" = list(name="UBCF", param=list(method = "Cosine")),  
  "Item Based CFPea" = list(name="IBCF", param=list(method = "Pearson")),
  "ALS" = list(name="ALS", param=list())
)

Multi_results <- evaluate(Multi_evaluate, 
                          method = algorithms, 
                          type = "topNList", 
                          parameter = list(normalize = "Z-Score"),
                          n=c(1, 5, seq(10,100,10)))

plot(Multi_results, annotate=c(1,2), legend = "topleft")

ratings_results <- evaluate(Multi_evaluate, algorithms, type = "ratings")

plot(ratings_results)

```

In the previous plots we can see the best results come from the POPULARITY algorithm.

In the ROC curve, we find a steep increase in True Positive Rates until the tenth recommendation. We would not recommend more than 10 movies considering a possible bigger marginal increment in False Positive Rates.

It is important to consider that the POPULARITY algorithm is based in the most popular items. From a business perspective, this may affect negatively the recommendations incurring in the banana problem (Only recommends top rating items). Considering this, we would create a Hybrid Recommendation Engine that includes Popularity and User Based Recommendations with a small number of nearest neighbors.

# Creating a Recomendation Engine





```{r}
df<-Groceries@data
```




You can also embed plots, for example:

```{r, echo=FALSE}

```

